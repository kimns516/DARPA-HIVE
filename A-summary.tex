\noindent
%Graph processing is an important tool to enhance our understanding of an increasingly complex and interconnected social world. 
%Inferring relationships between entities (or social actors), particularly going beyond the obvious edges between vertices one may see in a graph, is a computationally challenging task. 
This proposal focuses on executing graph algorithms as linear algebraic computations. 
This approach represents graphs as adjacency matrices, and many well known graph algorithms are transformed into matrix operations. 
The %evolving 
duality between graph algorithms and their equivalent matrix operations forms a solid basis for designing computing systems that can efficiently manipulate matrix operations. 

\vspace{3pt}
\noindent
\textbf{Approach and Cross-layer Innovations.} 
We propose to build a GAMA (Graph Acceleration through Matrix Abstraction) system to tackle critical challenges of graph processing. 
%an accelerator architecture, called 
Especially, given that graph algorithms place an enormous stress on the memory sub-system, 
we will develop the GAMA tile, a processing-in-memory tile where an accelerator is 2.5D-stacked with 4$\times$8GB of DiRAM4 dies.
% from Tezzaron. 

First, to efficiently support a broad range of matrix operations, we aim to develop a many-core graph acceleration architecture. 
In this architecture, each low-power core is tightly coupled with 8-lane sparse matrix acceleration engines and the memory sub-systems through network-on-chip.
Second, exploiting the DiRAM4's unique properties, we aim to redesign the memory sub-system (the on-chip L1 and L2 caches and off-chip memory controller) with innovative circuit and architecture techniques. 
This memory sub-system will efficiently support both sparse/random and dense/sequential memory accesses for efficient matrix operations. 
Lastly, we will design a high-speed link circuit with innovative charge-based and injection-locking techniques. 
This link will power-efficiently provide the target 1TBps bandwidth for inter-tile interconnections considering the tight chip power constraint.


To accomplish the target performance improvement of 1000$\times$ with more GAMA tiles, %we will further enhance the GAMA system. 
we will first enhance the memory sub-system that allows a GAMA accelerator to leverage 
%additional metadata information 
stored semantic information related to the matrix representation and expected access patterns to a given matrix.   
Second, we will build multiple enabling technologies to scale graph algorithms to 16 GAMA tiles and beyond. 
Especially, we will implement hardware counters and hooks that assist software to efficiently balance the load and relax synchronization.
%to measure load imbalances across GAMA tiles and provide an interface for software to decide on load migration strategies. 
%to monitor barrier synchronization stalls and allow the application developer to specify when it is acceptable for a GAMA tile to elide a barrier. 


%Using these high-speed links we will interconnect 16 GAMA tiles in a GAMA system. 
%an elastic cache architecture that 
% of providing many narrow memory channels. 
%Our elastic cache design 


To implement the GAMA system with innovative circuit techniques and architectures, 
we will work with Tezzaron, eSilicon, Nallatech, and IBM on this project. 
Tezzaron will provide the DiRAM4 dies and support an integration path for graph processor including a memory controller soft IP that our team can enhance.
eSilicon will work with our team to provide the chip fabrication and 2.5D integration services.
Nallatech design a PCB that mounts 16 GAMA tiles and connects to a IBM P8 host system with our team, and fabricate it.
IBM agreed to provide OpenCAPI interface specification and IPs for integrating the GAMA tiles with IBM P8 host processors. 

\vspace{3pt}
\noindent
\textbf{Success Metrics.} 
We will use the following metrics for evaluations.
We aim to achieve \textbf{X} GOPS/s for the performance of a GAMA tile, and \textbf{Y} TOPS/s for a GAMA system, while
each GAMA tile consumes no more than 20W.
%and energy consumption of \textbf{XX} nanoJoules per floating point multiply accumulate operation. 
For the memory sub-system, we aim to utilize 95\% of 4TBps peak bandwidth of DiRAM4 with 50\% improvement in L2 cache hit rates.
% with elastic cache, 
%and average memory access latency of \textbf{XX} cycles, and less than \textbf{1\%} memory overhead to store the metadata.
For the inter-tile interconnect we expect to achieve per lane power efficiency in the order of 1mW/Gb/s to deliver the 1TBps link bandwidth. % of 1TBps between two tiles. 
%We will provide a remote memory access latency of \textbf{XX} cycles for cross-tile data transfer. 
Through active monitoring of load imbalance and workload migration we expect to achieve 90\% scaling efficiency as we scale from 1 to 16 tiles. 
When the programmer is able to use relaxed synchronization appropriately we expect to gain super-linear performance scaling without any loss in accuracy or increased.  
Note that these metrics are evaluated on simulators in Phase 1, FPGA on Phase 2 and on a real system by Phase 3.  

%MURALI: DO WE NEED TO SAY ANYTHING ABOUT THE CHIP ITSELF IN TERMS OF TIMING, NOISE, GUARD BANDS ETC.,

\vspace{3pt}
\noindent
\textbf{Per Phase Cost.} 
Phases 1, 2, and 3 cost XX, YY, and ZZ million dollars, respectively.

%MURALI FILL ALL XXs. ALSO CHECK IF OUR PARTNERSHIP STUFF WITH COMPANIES IS CORRECT.
