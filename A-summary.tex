\noindent
%Graph processing is an important tool to enhance our understanding of an increasingly complex and interconnected social world. 
%Inferring relationships between entities (or social actors), particularly going beyond the obvious edges between vertices one may see in a graph, is a computationally challenging task. 
This proposal focuses on executing graph algorithms as linear algebraic computations~\cite{graph:primitives}. 
This approach represents graphs as adjacency matrices, and many well known graph algorithms can be transformed into matrix operations. 
The evolving 
%duality between graph algorithms and their equivalent matrix operations forms a solid basis for designing computing systems that can efficiently manipulate matrix operations. 
duality between graph algorithms and their equivalent matrix operations forms a solid basis 
for designing an efficient graph analytic processor. 

\vspace{3pt}
\noindent
\textbf{Approach and Cross-layer Innovations.} 
In this project, we aim to deliver 1000$\times$ higher system performance than the state-of-the-art GPU operating on sparse matrices.
Toward the goal, we propose to build the GAMA (Graph Acceleration through Matrix Abstraction) system to tackle critical graph processing challenges. 
The GAMA tile is a novel processing-in-memory architecture where a matrix computation accelerator is 2.5D-stacked with 4$\times$8GB DiRAM4 dies.
The accelerator is composed of multiple GAMA cores, where each core is configured as an 8-lane SIMD engine customized for matrix operations. Each core 
is provisioned with an L1 and a shared eDRAM L2 \emph{elastic caches} that support variable-width cache lines.  
The elastic cache design exploits the unique properties of DiRAM4 which supports many more narrow width memory channels than traditional high bandwidth memory.
This memory sub-system will efficiently support both sparse/random and dense/sequential memory accesses for efficient matrix operations. 
Lastly, we will design a high-speed link with state-of-the-art charge-based and injection-locking circuit techniques, which will allow us to provide the target 1TBps bandwidth for inter-chip communications under the tight chip power constraint.

For improving scalability the GAMA memory system will store the semantic information related to the matrix representation, and expected access patterns of a given matrix. Several hardware hooks will monitor  load imbalances across the GAMA tiles and provide an interface for software to decide on load migration strategies. We also plan to monitor barrier synchronization stalls and allow the application developer to specify when it is acceptable for a GAMA tile to elide a barrier. 

%Using these high-speed links we will interconnect 16 GAMA tiles in a GAMA system. 
%an elastic cache architecture that 
% of providing many narrow memory channels. 
%Our elastic cache design 


To implement the GAMA system, % with innovative circuit and architecture techniques, 
we will work with 
four companies on this project. 
Tezzaron will supply the DiRAM4 dies and support an integration path for graph processor including a memory controller soft IP that our team can enhance.
eSilicon will provide the chip fabrication and 2.5D integration services.
Nallatech will design a PCB that mounts 16 GAMA tiles and connects the tiles to the IBM OpenPower host system. % with our team, and fabricate it.
IBM will support OpenCAPI interface specification and IPs for integrating the GAMA tiles with IBM OpenPower host processors. 

\vspace{3pt}
\noindent
\textbf{Success Metrics.} 
We will use the following metrics for evaluations.
We aim to achieve 1 Tera/0.5Tera ops/s for integer and floating-point sparse matrix operations, respectively, per a GAMA tile while each GAMA tile consumes no more than 20W.
For the memory sub-system, we aim to utilize 95\% of 4TBps peak bandwidth of DiRAM4 with 50\% improvement in L2 cache hit rates with our elastic cache architecture.
%and average memory access latency of \textbf{XX} cycles, and less than \textbf{1\%} memory overhead to store the metadata.
For the inter-chip interconnect we expect to achieve per-lane power efficiency in the order of 1mW/Gb/s to deliver the 1TBps link bandwidth. % of 1TBps between two tiles. 
%We will provide a remote memory access latency of \textbf{XX} cycles for cross-tile data transfer. 
Through active monitoring of load imbalance and workload migration we expect to achieve 95\% scaling efficiency as we scale from 1 to 16 tiles. 
When the programmer exploits relaxed synchronization support provided in hardware  we expect to gain super-linear performance scaling with limited accuracy and performance tradeoff.  
Note that these metrics are evaluated on simulators in Phase 1, FPGA on Phase 2 and on a real system by Phase 3.  

%MURALI: DO WE NEED TO SAY ANYTHING ABOUT THE CHIP ITSELF IN TERMS OF TIMING, NOISE, GUARD BANDS ETC.,

\vspace{3pt}
\noindent
\textbf{Per Phase Cost.} 
Phases 1, 2, and 3 cost \$1,064,549, \$6,864,667, and \$2,328,572, respectively.

%MURALI FILL ALL XXs. ALSO CHECK IF OUR PARTNERSHIP STUFF WITH COMPANIES IS CORRECT.
