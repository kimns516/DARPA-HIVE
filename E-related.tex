\noindent
The memory sub-system of state-of-the-art GPUs and Intel Knights Landing~\cite{knight:intel:2016} 
are based on HBM~\cite{hbm} and HMC~\cite{hmc}. These are optimized for applications with sequential/dense memory accesses, resulting in significant memory bandwidth wastage when dealing with sparse matrices.
A recent NVIDIA GPU, Titan can deliver only 5 Giga ops/s for sparse matrix operations~\cite{liu2014efficient}, although its peak performance is almost 5 Tera ops/s for single precision operations. The  degradation is due to long memory stalls and inefficient use of available memory bandwidth.  

Tesseract is a recently proposed graph accelerator that uses processing-in-memory to improve memory bandwidth utilization~\cite{ahn2015scalable}.  Tesseract uses message passing to communicate across cores that are integrated into memory, and uses  prefetchers that are driven by programmer hints. Tesseract  is a single node system. GAMA is a scaled system that supports efficient sparse and dense matrix operations through specialized caches. GAMA also uses prefetchers to prefetch data based on column buffers. Graphicianado~\cite{Ham2016}  is another graph analytics accelerator that uses vertex based processing  where each vertex is visited once every iteration. They optimized the memory system for vertex traversals. GAMA on the other hand relies on accelerating sparse matrix operations to implement graph algorithms and also tackle the challenge of scaling to multiple cores. A configurable graph accelerator built on top of SystemC was also proposed recently~\cite{ozdal2016energy}. The graph algorithm is written in SystemC and a compiler will automatically generate the accelerator code. FPGP is a FPGA-based accelerator that mapped BFS algorithm to FPGAs~\cite{dai2016fpgp}. 

On the software front several efforts are being made to distribute graph analytics for parallel execution. For instance, Presto uses enhancements to "R" language to implement distributed graph analytics~\cite{venkataraman2013presto}. GraphCHI allows graph analytics on desktop PCs through graph tiling on disks, but it is only able to provide a fraction of the throughput that GAMA aims for. Chronos optimizes graph analytics for in-memory graphs using careful graph data layout in memory and a careful orchestration of computations and data accesses~\cite{han2014chronos}. Trinity targets graph analytics on RAMcloud where the analytics are built on a domain specific language TSL~\cite{shao2013trinity}. Powergraph is a specialized graph processing framework that targets graphs that exhibit power law distributions. Our goal is to accelerate graph algorithms written in any of these software frameworks efficiently. GAMA provides additional hardware hints to enable Trinity or Powergraph to map graphs to GAMA accelerator. 

Over/under-drives have been applied to SRAM~\cite{7046972, 6176988} and eDRAM~\cite{6757412} to reduce read/write failure. However, the over/under-drive amounts are small (typically $<$ 200mV) and were not applied to  peripheral circuits in the past. These prior approaches are different from the spatio-temporal boosting techniques proposed here which aims to reduce access latency. Due to eDRAM's sensitivity to various failure mechanisms our proposed approach requires careful thermal monitoring and error detection schemes that we will be developing for GAMA. High bandwidth link design challenges were tackled in the past, particularly in the context of supporting GDDR memory interface to GPUs~\cite{partovi2009single, kho201075}. In~\cite{krishnaswamy2013bandwidth}, power efficiency was improved by customizing link architectures in accordance with data rate and channel characteristics. Using techniques such as analog equalization, baud-rate clock recovery, a power efficiency of 11mW/Gb/s is achieved while supporting 2.9TB/s throughput. At this rate, the total power consumption at 1TB/s is is in excess of 88W, which far exceeds the target 20W power constraint. Our own prior work has shown the importance of using charge-based techniques and injection locking~\cite{saxena20152, elkholy201610}. The link design on GAMA is in fact built on top of these successful prior studies. But we recognize an important distinction, namely link power efficiency cannot be improved through dynamic power management schemes that try to save power during low bandwidth usage phases.   Since GAMA tiles are designed to minimize processor idle time,  our proposed link designs improve power efficiency even at a peak data rate. To summarize, compared to many prior works  GAMA uses  new accelerator architecture customized for sparse and dense  matrix operations,  integrated with DiRAM4 memory. We will build a highly scalable system with 16 GAMA tiles connected using high speed links and hardware hooks for efficient programming.  

%In improving link power efficiency, dynamic power management techniques (e.g., \cite{anand:jssc:2015} cannot be used to improve power efficiency, because the GAMA tiles are going to be designed to minimize processor idle time.  Instead, the techniques that improve power efficiency at a peak data rate as Sec.~\ref{sec:scale:link} are needed.


\begin{comment}
Various spatio-temporal voltage boosting techniques have been applied to SRAM to reduce read/write failure of SRAM arrays at low voltage~\cite{}, 
while the failure mechanism of eDRAM arrays singificantly differs from that of SRAM arrays.
That is, such voltage boosting techniques have never been applied to eDRAM arrays especially to reduce the access latency.
\end{comment}


\begin{comment}
HIVES calls for creation of processor-to-memory and processor-to-processor communication interfaces that can support extraordinarily high bandwidth exceeding 1TB/s or equivalently 8Tb/s. This requires a total of either 800 lanes operating at 10Gb/s/lane or 320 lanes operating at 25Gb/s/lane.  With a per-lane power efficiency of 10mW/Gb/s, the total power consumption will be in excess of 80W, which greatly complicates both the thermal design and power generation/distribution. Recognizing that large portion of the power consumed in multi-lane transceivers is used to perform equalization, clock generation/recovery/distribution and serializer/deserializer functions, recent work focussed on developing low power circuit techniques to implement these functions \footnote{Zhang et al. JSSC 2015}.   
In our recent work (Saxena et al., VLSIC 2015), we utilized charge-based techniques (as opposed to commonly used voltage/current based approaches) to implement equalization (both continuous-time and decision feedback), serialization and deserialization to improve link power efficiency to about 3mW/Gb/s, which represents more than 3$\times$ improvement. 
In \footnote{Elkholy et al., ISSCC 2016}, we demonstrated ultra low power clock generation methods using injection locking. 
At 8GHz, the prototype clock multiplier achieves about $\rm 100 fs_{rms}$-integrated jitter while consuming only 2mW, resulting in more than 5dB improvement in the jitter/power figure-of-merit. 
Using this technique our most recent work (Nandwana et al., ISSCC2017) demonstrated  extremely power efficient clock generation/distribution/recovery techniques for multi-lane transceivers. 
We foresee that these techniques will become instrumental in achieving 1mW/Gb/s power efficiency, which greatly helps realization of 1TB/s interconnect bandwidth practical. 
\end{comment}



\begin{comment}
%%MS EDIT START%%
%%Fine-grained spatiotemporal boosting 
\textbf{Spatiotemporal Boosting in Embedded Memory:}
While static multi voltage designs are popular in embedded memory circuits, 
only recently spatiotemporal voltage boosting has received large attentions. 
Multiple works have demonstrated overdrive and underdrive in bitlines, wordlines, and ground potentials, and voltage collapsing in bitcells. 
These works focus on improving readability and writability of deeply-scaled bitcells at low supply voltage\footnote{XX}. 
A very recent work explores the feasibility to boost supply voltage of sub-VT SRAM for improving writability but the granularity of applying voltage boosting is coarse. 
This work also focuses on improving writability. 
In our recent work\footnote{J. P. Cerqueira, ISSCC'17 pending}, we created spatiotemporal voltage boosting to speed up the access of ultra-low-leakage SRAM bitcells for a 0.35V ultra-low-energy and area-efficient FFT core. It is for improving latency and throughput in memory access, however, the boosted level of supply voltage is still slightly higher than $V_T$ (about 0.6V). Ref.\footnote{XX} explores power-grid design to rapidly boost supply voltage of a microprocessor core. However, the work focuses on coarse granuality and also incurs non-negligible on-chip capacitor (XX\% of the area of the load to be boosted) to mitigate voltage droops and ripples. 

%%EDAC
\textbf{EDAC for Parallel and Non-Von-Neumann Architectures:} While among the first EDAC techniques were proposed in 2002\footnote{}-2006\footnote{}, till now, most of them focus on in-order RISC processors in Von-Neumann architectures. Applying those EDAC to other architectures, such as parallel architecture, specialized accelerator architectures, and embedded memory, is neither straightforward nor area/energy-efficient. For example, instruction replay is a popular scheme to correct errors but it cannot be used in architectures without instructions without a prohibitive amount of overhead for saving a check point to roll back states. In our recent work on neural network accelerator design, we analyzed the overhead of check points can be as large as duplicating XX\% of registers in designs, a huge overhead. We have crated the EDAC that can detect and correct errors for architectures without instruction equivalent by using spatiotemporal voltage boosting\footnote{Kim et al., VLSI14 \& JSSC'15} and body swapping\footnote{Kim et al., VLSI'16} which incurs only 4-8\% area overhead. A recent work\footnote{Fojtik et al., ISSCC'12} created EDAC with local clock-gating based correction but it incurs large area overhead (up to 80\%). 

In addition, minimizing the overhead associated with error detection has been also critical. Some works created new error-detecting circuits using a small number of transistors\footnote{Kim et al., ISSCC12, XX et al., ISSCC15}. Our recent work created a framework to skip several pipeline stages to insert error-detecting circuits without compromising error detection coverages. This technique is based on two-phase latch sequencing and its cycle-borrowing capability\footnote{Kim et al., ISLPED'14, Jin et al., TVLSI'16 (pending)}. Recently we have also explored the use of pulsed-latch sequencing with an area-efficient short-path padding scheme, which incurs less overhead than two-phase latch sequencing but provides a comparable amount of cycle-borrowing window \footnote{Jin et al., ASSCC'16}. 

%%DTM
\textbf{Dynamic Thermal Management in Today's Thermal-Limited Systems:} 
Today, VLSI systems are thermally limited. On the high-performance side, temperature limits clock frequency scaling and also has an adverse impact on reliability, cooling, package, and leakage. These challenges make many systems to have dynamic thermal management (DTM), which has been successful for the last decade. But, the technology trends continue to stress thermal issues. For example, FinFETs are replacing planar devices. But, they have oxide substrates, causing localized hotspots. Also, 3DIC and other integration efforts make a chip producing more heat at smaller space. What's worse, more and more thermally-sensitive circuits seek near-chip integration, such as stacked cache, DRAM, NVM, and on-chip photonics. In the DTM systmes, accurate hotspot detection affects the overall quality. However, studies including ours show large error in hotspot detection. $\sim$75\% of the error is attributed to non-negligible distance between a sensor and a hotspot and $\sim$25\% is sensor's inherent error. To tackle this, it is key to place sensors \textit{more closely to hotspots}. However, existing sensors are too invasive for such placement. They often take >$1,000$'s $\mu m^2$ each and need $V_{DD}$>1V (See Fig.~\ref{XX} for our surveys on area and voltage scalability). We need to scale size and minimum $V_{DD}$ (for sharing digital $V_{DD}$), but achieving high accuracy under digital noise.

%%MS EDIT END%%
\end{comment}

