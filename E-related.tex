\noindent
The memory sub-system of state-of-the-art GPUs and as such (e.g., Intel Knights Landing) based on HBM and HMC is optimized for applications with sequential/dense memory accesses, wasting memory bandwidth.
Thus, these processors cannot reap the benefit of high (sequential access) bandwidth and underutilize their processing cores for graph algorithms with random/sparse memory accesses.
Some accelerators are proposed to efficiently process sparse matrix operations, but they are built on the conventional memory architecture, sufferring from the same issue as the GPUs and as such.
In contrast, using the state-of-the-art, soon-to-be-released memory, we propose to jointly architect accelerator and memory sub-system architectures 
such that the accelerator system can efficiently execute both dense and sparse matrix operations.

Various spatio-temporal voltage boosting techniques have been applied to SRAM to reduce read/write failure of SRAM arrays at low voltage, while the failure mechanism of eDRAM arrays singificantly differs from that of SRAM arrays.
That is, such voltage boosting techniques have never been applied to eDRAM arrays especially to reduce the access latency.

HIVES calls for creation of processor-to-memory and processor-to-processor communication interfaces that can support extraordinarily high bandwidth exceeding 1TB/s or equivalently 8Tb/s. This requires a total of either 800 lanes operating at 10Gb/s/lane or 320 lanes operating at 25Gb/s/lane.  With a per-lane power efficiency of 10mW/Gb/s, the total power consumption will be in excess of 80W, which greatly complicates both the thermal design and power generation/distribution. Recognizing that large portion of the power consumed in multi-lane transceivers is used to perform equalization, clock generation/recovery/distribution and serializer/deserializer functions, recent work focussed on developing low power circuit techniques to implement these functions \footnote{Zhang et al. JSSC 2015}.   
In our recent work (Saxena et al., VLSIC 2015), we utilized charge-based techniques (as opposed to commonly used voltage/current based approaches) to implement equalization (both continuous-time and decision feedback), serialization and deserialization to improve link power efficiency to about 3mW/Gb/s, which represents more than 3$\times$ improvement. 
In \footnote{Elkholy et al., ISSCC 2016}, we demonstrated ultra low power clock generation methods using injection locking. 
At 8GHz, the prototype clock multiplier achieves about $\rm 100 fs_{rms}-integrated jitter while consuming only 2mW, resulting in more than 5dB improvement in the jitter/power figure-of-merit. 
Using this technique our most recent work (Nandwana et al., ISSCC2017) demonstrated  extremely power efficient clock generation/distribution/recovery techniques for multi-lane transceivers. 
We foresee that these techniques will become instrumental in achieving 1mW/Gb/s power efficiency, which greatly helps realization of 1TB/s interconnect bandwidth practical. 



\begin{comment}
%%MS EDIT START%%
%%Fine-grained spatiotemporal boosting 
\textbf{Spatiotemporal Boosting in Embedded Memory:}
While static multi voltage designs are popular in embedded memory circuits, 
only recently spatiotemporal voltage boosting has received large attentions. 
Multiple works have demonstrated overdrive and underdrive in bitlines, wordlines, and ground potentials, and voltage collapsing in bitcells. 
These works focus on improving readability and writability of deeply-scaled bitcells at low supply voltage\footnote{XX}. 
A very recent work explores the feasibility to boost supply voltage of sub-VT SRAM for improving writability but the granularity of applying voltage boosting is coarse. 
This work also focuses on improving writability. 
In our recent work\footnote{J. P. Cerqueira, ISSCC'17 pending}, we created spatiotemporal voltage boosting to speed up the access of ultra-low-leakage SRAM bitcells for a 0.35V ultra-low-energy and area-efficient FFT core. It is for improving latency and throughput in memory access, however, the boosted level of supply voltage is still slightly higher than $V_T$ (about 0.6V). Ref.\footnote{XX} explores power-grid design to rapidly boost supply voltage of a microprocessor core. However, the work focuses on coarse granuality and also incurs non-negligible on-chip capacitor (XX\% of the area of the load to be boosted) to mitigate voltage droops and ripples. 

%%EDAC
\textbf{EDAC for Parallel and Non-Von-Neumann Architectures:} While among the first EDAC techniques were proposed in 2002\footnote{}-2006\footnote{}, till now, most of them focus on in-order RISC processors in Von-Neumann architectures. Applying those EDAC to other architectures, such as parallel architecture, specialized accelerator architectures, and embedded memory, is neither straightforward nor area/energy-efficient. For example, instruction replay is a popular scheme to correct errors but it cannot be used in architectures without instructions without a prohibitive amount of overhead for saving a check point to roll back states. In our recent work on neural network accelerator design, we analyzed the overhead of check points can be as large as duplicating XX\% of registers in designs, a huge overhead. We have crated the EDAC that can detect and correct errors for architectures without instruction equivalent by using spatiotemporal voltage boosting\footnote{Kim et al., VLSI14 \& JSSC'15} and body swapping\footnote{Kim et al., VLSI'16} which incurs only 4-8\% area overhead. A recent work\footnote{Fojtik et al., ISSCC'12} created EDAC with local clock-gating based correction but it incurs large area overhead (up to 80\%). 

In addition, minimizing the overhead associated with error detection has been also critical. Some works created new error-detecting circuits using a small number of transistors\footnote{Kim et al., ISSCC12, XX et al., ISSCC15}. Our recent work created a framework to skip several pipeline stages to insert error-detecting circuits without compromising error detection coverages. This technique is based on two-phase latch sequencing and its cycle-borrowing capability\footnote{Kim et al., ISLPED'14, Jin et al., TVLSI'16 (pending)}. Recently we have also explored the use of pulsed-latch sequencing with an area-efficient short-path padding scheme, which incurs less overhead than two-phase latch sequencing but provides a comparable amount of cycle-borrowing window \footnote{Jin et al., ASSCC'16}. 

%%DTM
\textbf{Dynamic Thermal Management in Today's Thermal-Limited Systems:} 
Today, VLSI systems are thermally limited. On the high-performance side, temperature limits clock frequency scaling and also has an adverse impact on reliability, cooling, package, and leakage. These challenges make many systems to have dynamic thermal management (DTM), which has been successful for the last decade. But, the technology trends continue to stress thermal issues. For example, FinFETs are replacing planar devices. But, they have oxide substrates, causing localized hotspots. Also, 3DIC and other integration efforts make a chip producing more heat at smaller space. What's worse, more and more thermally-sensitive circuits seek near-chip integration, such as stacked cache, DRAM, NVM, and on-chip photonics. In the DTM systmes, accurate hotspot detection affects the overall quality. However, studies including ours show large error in hotspot detection. $\sim$75\% of the error is attributed to non-negligible distance between a sensor and a hotspot and $\sim$25\% is sensor's inherent error. To tackle this, it is key to place sensors \textit{more closely to hotspots}. However, existing sensors are too invasive for such placement. They often take >$1,000$'s $\mu m^2$ each and need $V_{DD}$>1V (See Fig.~\ref{XX} for our surveys on area and voltage scalability). We need to scale size and minimum $V_{DD}$ (for sharing digital $V_{DD}$), but achieving high accuracy under digital noise.

%%MS EDIT END%%
\end{comment}

