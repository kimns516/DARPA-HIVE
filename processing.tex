\noindent
The basic building block of the GAMA accelerator (Fig.~\ref{fig:arch}(b)) is the GAMA core,
a RISC-V processor  tightly coupled with an 8-lane compute engine to efficiently accelerate sparse matrix operations (Fig.~\ref{fig:arch}(c)).
The core is architected to provide 16GOPs at 34mW and the estimated area is 0.5mm$^2$.
We will first describe the microarchitecture of the GAMA core, and then describe the overall architecture of the GAMA accelerator.


It has been shown that a wide range of graph algorithms can be implemented by a general case of matrix multiply \textbf{C} = \textbf{A} \textit{f}().\textit{g}() \textbf{B}~\cite{graph:primitives}. 
For example, a matrix-vector multiply (i.e., \textbf{C} = \textbf{A} +.$\times$ \textbf{B}) represents an one-hop BFS 
where \textbf{A} is a $N \times N$ connectivity matrix, \textbf{B} is a $1 \times N$ vector of the starting node, 
\textit{g}($i$, $j$) returns $a_{i,j} \times b_{j}$ and \textit{f}($i$) returns $\sum_{j=1}^{N}$ \textit{g}($i$,$j$).


As large graphs are often represented by very sparse matrices, compressed formats, such as double compressed sparse column and row (DCSR and DCSC) formats, are widely used to efficiently store the matrices in terms of storage size.
However, the storage compaction complicates matrix operations and requires irregular memory accesses especially for constructing matrix \textbf{C} in a compressed format on the fly (i.e., merging operations).
After carefully analyzing the state-of-the-art DCSC and DCSR formats and matrix multiplication algorithms based on these formats, 
we propose the GAMA core microarchitecture depicted in Fig.~\ref{fig:arch}(c). The GAMA core avoids storing \textbf{A} and \textbf{B} in multiple formats, which can simplify the accesses of  non-zero values (NZVs) for matrix operations.
Our microarchitecture can handle both row- and colum-major formats, but for discussion clarity here let us assume  \textbf{A} and \textbf{B} are stored in DCSC or a similar column-major format.
In such a format, NZVs in a given column are already sorted in row order and stored in an array structure sequentially. 
To stage NZVs with their row indices from both \textbf{A} and \textbf{B} for parallel operations, we propose A- and B-column buffers. 

Each buffer can consist of one or more 64-byte entries, each of which can be loaded by one special load instruction (\texttt{LD64B}) that we will implement as an extended RISC-V ISA. 
In Fig.~\ref{fig:arch}(c) ``A-column buffer'' has 8 entries, whereas ``B-column buffer'' has one entry.
Note that \texttt{LD64B} will bypass L1D cache and directly access the GAMA main memory, as these buffers serve as cache for values and their row indices of \textbf{A} and \textbf{B}.
Each entry of a buffer stores 8 pairs of a NZV and its row index from a column of a matrix, and
%we store the memory address associated with the column index of all 8 pairs in a register in the RISC-V processor.
we store the column index of all 8 pairs in a register of the RISC-V processor.
%the number of NZVs in a column is much larger than that of 8 in an acceleration engine in large matrices of project's interest, and thus
%Each buffer has entries, and each entry stores a two tuple, the numerical value of the matrix element, and the corresponding row index. 
Note that each buffer entry is associated only with a single column, 
as the number of NZVs in a column is much larger than 8 in large sparse matrices of project's interest. 


Consider ``B-column buffer'' in Fig.~\ref{fig:arch}(c), where
%The row index determines which column in \textbf{A} is associated with the given value. 
%Let us assume that the row index of this buffer is one. 
%Let us assume that the row index of the first (bottom most) pair in B-column buffer is ``1''.
the row index of the first (bottom most) pair in B-column buffer is ``1.''
The row index of \textbf{B} determines the column index of \textbf{A} for matrix multiplications. % stored in DCSC or a similar column-major format.
We now need to find the values of the corresponding column of \textbf{A} to perform multiplications.  
For a given column, the column-major sparse matrix format allows us to easily retrieve 
the starting address and the number of NZVs sequentially stored in the array. 
After the processor determines the starting address and the number of NZVs of the corresponding column, 
it also loads 8 pairs of a NZV and its row index in an entry of A-column buffer by executing a \texttt{LD64B} instruction.


As depicted in Fig.~\ref{fig:arch}(c), there are 8 ALUs for \textit{g}(), and the value in the bottom-most (first) pair in the B-column buffer 
%(corresponding to the bottom most entry in the )  
is broadcast to all 8 ALUs as one of the operands for each ALU.
The 8 NZVs in the first entry of the A-column buffer are fed to the 8 ALUs as the other operand for each ALU.
We will use ALUs supporting single-precision floating-point multiply-add operations for weighted edge graphs. 
We also plan to optimize the \textit{g}() function circuit design to efficiently support graphs with non-weighted edged (``1'' implies an edge and ``0'' implies no edge). 
For non-weighted edge graphs we expect to derive a significant power reduction by optimizing the multiplication with 1 and 0 values, 
allowing the GAMA accelerator to operate at higher frequency under the power contraint.     
 
%However, all the NZVs will be ``1'' and a multiplication can be replaced with a ``AND'' operation for graph traversing problems. In such a case, we propose to power-gate the floating-point unit in the ALU and run the core at higher frequency than 2GHz, as AND operations consume much less power than floating-point operations and less contrained by power and thermal issues. 


The 8 computed results with the corresponding 8 row indices from the first entry of A-column buffer will be applied to 
the ``parallel memory access unit'' in Fig.~\ref{fig:arch}(c) with 
(1) the column index of NZVs in a processor register and (2) the starting address of the heap associated with the column index of B-column buffer.
The heap is used to construct $c_{i,k} in $\textbf{C} from $a_{i,j} \times b_{j,k}$ for a range of $j$ (i.e., merging operations)~\cite{buluc:ipdps:2015}. 
As the access pattern of the heap is very random, these 8 values with random addresses will be stored to and retrieved from our elastic L1D cache (cf. Sec.~\ref{sec:memory:on-chip}) with 8 banks to support 8 concurrent accesses to 8 different random addresses.
Moreover, we will use the ALUs to support three-operand computations such as multiply-add, to efficiently support the accumulation of 
the computed values to the corresponding values of \textbf{C} stored in the heap.


After all the NZVs of column ``1'' of \textbf{A} is consumed in Fig.~\ref{fig:arch}(c), 
the head pointer of the B-column buffer operating as a FIFO buffer points to the second pair.
%makes the second pair from the bottom most one as the first pair, 
In Fig.~\ref{fig:arch}(c), the row index of the second pair is ``11,'' 
and thus the processor brings the NZVs and row indices of column ``11'' of \textbf{A}. 
%While the computation with one row index entry of a B-column buffer progresses, 
While the computation associated with one row index of B-column buffer progresses, 
the subsequent row indices stored in B-column buffer will be used for prefetching the NZVs and corresponding row indices of the next columns of \textbf{A} to the A-column buffer.


The RISC-V processor has 4KB IL1 and 32KB 8-bank DL1 caches.
The processor will be enhanced to efficiently perform various management tasks for the acceleration engine such as:
calculating the starting memory addresses of arrays storing NZVs 
and their associated row values; and
controlling the data-flow of the acceleration engine for merging operations when constructing \textbf{C} through the heap.
 

%Then it issues an instruction that brings 64 bytes containing 8 pairs of row index and associated value and store them . 
%is to process elements from a (subset of) row of \textbf{A} and a (subset of) column of \textbf{B} (Fig.~\ref{fig:acc-arch}(b)).
%This is advantageous for a given \textit{f}($i$) to efficiently operate on \textbf{g}($i$, $j$).
%For efficient execution of complex \textbf{g}() and \textbf{f}(), we propose to use 

The architecture of a GAMA accelerator is composed of four MIMD clusters, and the cluster architecture is depicted in Figure~\ref{fig:arch}(b).
The four cluster consists of a total of 16 cores operating at 2GHz or higher and connected to a special 512KB eDRAM-based L2 cache (cf. Sec.~\ref{sec:memory:on-chip}) and a 16GBps on-chip interconnect between them. 
An eDRAM L2 cache can be connected to one of 16 DiRAM4 memory controllers, each of which is connected to a 64-bit (16GBps) DiRAM4 memory channel, through switches.
Moreover, a core can send a memory request to a port of the OpenCAPI link consisting of 16 ports and providing 256GBps aggregate bandwidth, 
as well as network on-chip (NOC) interconnects to three other clusters.  
%which are connected to each other through on-chip network.

%32 corelets are placed on a single die and share a single large L2 cache designed using eDRAM. The L2 cache is then backed up by 4X8GB DiRAM4 memory modules. DiRAM4 is a die-stacked DRAM die that is provided by Tezzaron (one of our service providers in this project). The 32 corelets are 2.5D stacked with DiRAM4. This entire package is called a GAMA tile. We will integrate 16 GAMA tiles to create the whole GAMA accelerator system. 16 GAMA tiles are interconnected  using our custom-designed 1Tbps inter-tile interface. The GAMA system is interfaced with IBM Power8-based host system using OpenCAPI interface. 
%At each layer of the above design we propose several innovative ideas. At the corelet layer we propose to design specialized address mapping engines that translate the sparse matrix indices to generate the row/column index of matrix elements based on the underlying sparse matrix format. While the 32 GAMA corelets can do MIMD computations they can  be dynamically reconfigured to execute a single instruction across a very wide vector range when accesses are highly sequential.  The elastic  cache attached to each corelet is architected to support both dense/sequential and sparse/random accesses to a matrix. The elastic cache is a low-overhead cache organization that supports variable width cache lines to improve cache usage efficiency.  
% The unique aspect of DiRAM4 memory, compared to other competing memories such as high bandwidth memory, is that each DiRAM4 die supports more channels per die, but uses narrow width channels. Such a design is well suited for graph workloads that need narrow width memory level parallelism. The narrow width channels can be reconfigured to create a single wide width channel to support dense matrices. The memory layer itself is made semantically aware of the matrix structure and its preferred lay out and access patterns in memory. The semantic awareness comes from application provided hints that are conveyed to the memory controller through APIs. Since graphs that are spread over multiple tiles demand high bandwidth inter-tile access. High-bandwidth links  posing power, thermal, and signal integrity challenges.  We tackle this challenge with various charge-based and injection-locking techniques for high-speed links. The scalability of parallel graph algorithms will be improved by enabling barrier elision hardware that can be used by the application developer to tradeoff accuracy with latency. We provide an in-depth visibility to software to monitor changes to the graph structure and take action to deal with potential load imbalances.  

%32 corelets are placed on a single die and share a single large L2 cache designed using eDRAM. The L2 cache is then backed up by 4X8GB DiRAM4 memory modules. DiRAM4 is a die-stacked DRAM die that is provided by Tezzaron (one of our service providers in this project). The 32 corelets are 2.5D stacked with DiRAM4. This entire package is called a GAMA tile. We will integrate 16 GAMA tiles to create the whole GAMA accelerator system. 16 GAMA tiles are interconnected  using our custom-designed 1Tbps inter-tile interface. The GAMA system is interfaced with IBM Power8-based host system using OpenCAPI interface. 
%At each layer of the above design we propose several innovative ideas. At the corelet layer we propose to design specialized address mapping engines that translate the sparse matrix indices to generate the row/column index of matrix elements based on the underlying sparse matrix format. While the 32 GAMA corelets can do MIMD computations they can  be dynamically reconfigured to execute a single instruction across a very wide vector range when accesses are highly sequential.  The elastic  cache attached to each corelet is architected to support both dense/sequential and sparse/random accesses to a matrix. The elastic cache is a low-overhead cache organization that supports variable width cache lines to improve cache usage efficiency.  
% The unique aspect of DiRAM4 memory, compared to other competing memories such as high bandwidth memory, is that each DiRAM4 die supports more channels per die, but uses narrow width channels. Such a design is well suited for graph workloads that need narrow width memory level parallelism. The narrow width channels can be reconfigured to create a single wide width channel to support dense matrices. The memory layer itself is made semantically aware of the matrix structure and its preferred lay out and access patterns in memory. The semantic awareness comes from application provided hints that are conveyed to the memory controller through APIs. Since graphs that are spread over multiple tiles demand high bandwidth inter-tile access. High-bandwidth links  posing power, thermal, and signal integrity challenges.  We tackle this challenge with various charge-based and injection-locking techniques for high-speed links. The scalability of parallel graph algorithms will be improved by enabling barrier elision hardware that can be used by the application developer to tradeoff accuracy with latency. We provide an in-depth visibility to software to monitor changes to the graph structure and take action to deal with potential load imbalances.  


\begin{comment}
\noindent
\textbf{Trailblazer:} 
One of the unique properties of graph algorithms is that all vertices perform the same computation (with some limited control flow support). 
Hence, the execution behavior of future vertex computations can be easily inferred from current vertex computations. 
We will use the notion of trailblazing where some of the vertex computation tasks are closely monitored. 
During trail blazing various microarchitectural events are monitored, such as memory bandwidth, branch behavior patterns and affinity of communication between different vertices. 
We hypothesize that there are pairs of vertices that exhibit dominant communication patterns which can be inferred from trail blazing. 
We will use the knowledge from trailblazing to potentially co-locate affine vertices to the same compute node. 
The notion of trail blazing was used for prefetching irregular data in CPUs, but we plan to apply similar principle for managing vertex computations.

\end{comment}
