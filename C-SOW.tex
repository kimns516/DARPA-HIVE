%1. A general description of the objective (for each defined task/activity);
%2. A detailed description of the approach to be taken to accomplish each defined task/activity;
%3. Identification of the primary organization responsible for task execution (prime, sub, team member, by name, etc.);
%4. The completion criteria for each task/activity - a product, event or milestone that defines its completion.
%5. Define all deliverables (reporting, data, reports, software, etc.) to be provided to the Government in support of the proposed research tasks/activities; AND
%6. Clearly identify any tasks/subtasks (prime or subcontracted) that will be accomplished on-campus at a university.

\noindent
\textbf{Phase 1.}

\noindent
All the tasks in Phase 1 will be on-campus at UIUC, USC, and Columbia.

\noindent
\textbf{Task 1: Modeling the processing pipeline.} 
The Annavaram's team at USC will model the proposed processing pipeline architecture in software using a programming language.
This includes architecting coarse-grain reconfigurable execution pipeline for accelerating graph primitives requiring complex operations.
This software code will be integrated with an architectural simulator (cf. Task 5) that integrates all the functions of the proposed accelerator.
When the software code can successfuly run the test code derived from given benchmarks and provide the estimation of performance in terms of simulated cycles, this task is completed.
We will deliver the completed software code, test code, and technical document describing the architecture, implementation in C++, and usage.

\noindent
\textbf{Task 2: Modeling the memory subsystem.}
The Kim's team at UIUC will model the proposed memory system architecture in software using a programming language such as C++.
This includes architecting (1) proposed custom SRAM-based on-chip caches private to each acceleration core; (2) a custom eDRAM-based on-chip cache shared by all the acceleration cores in a chip; and (3) a custom memory controller.
This software code will be integrated with an architectural simulator (cf. Task 5) that integrates all the functions of the proposed accelerator.
When the software code can successfuly run the test code derived from given benchmarks and provide the estimation of performance in terms of simulated cycles, this task is completed.
We will deliver the completed software code, test code, and technical document describing the architecture, implementation in C++, and usage.

\noindent
\textbf{Task 3: Designing eDRAM array circuit schematic.}
The Seok's team at Columbia will design an eDRAM array circuit in a 28nm CMOS technology.
This includes the development of circuit structures that (1) support spatial and temporal boosting and (2) non-invasive thermal sensors and a dynamic thermal management scheme to ensure that the spatio and temporal boosting does not violate a thermal limit.
When HSPICE can successfully simulate the the designed eDRAM array circuit and demonstrate the projected latency reduction compared with an eDRAM without the boosting technique, this task is completed.
We will deliver the completed eDRAM circuit and testbench netlists, performance parameters needed for the architectural simulation, and technical document describing the design and HSPICE simulation results.

\noindent
\textbf{Task 4: Designing 1TBps link circuit.}
The Hanumolu's team at UIUC will design a 1TBps link circuit in a 28nm CMOS technology.
This includes the development of circuit structures that employ (1) injection-locking and (2) clocking techniques that improve power and area efficiency. 
When HSPICE can successfully simulate the the designed link circuit and demonstrate the projected power-efficiency improvement compared with the state-of-the-art link circuit, this task is completed.
We will deliver the completed link circuit and testbench netlists, performance parameters needed for the architectural simulation, and technical document describing the design and HSPICE simulation results.

\noindent
\paragraph{Task 5: Modeling the entire accelerator module.}
The Hwu's team at UIUC will model the proposed intra- and inter-chip communication architectures and integrate the processing pipeline (Task 1) and memory subsystem (Task 2) models with the communication architectures to model the entire accelerator module in software using a programming language. 
When the simulator software code can successfuly run the test code derived from given benchmarks with the input set fitting into the on-package memory system comprised of 4$\times$ 8GB DiRAM4 dies, and provide the estimation of performance in terms of simulated cycles, this task is completed.
We will deliver the completed simulator code, test code, and technical document describing the architecture, implementation in C++, and usage.


\paragraph{Phase 2.}

\noindent
\paragraph{Task 6: .}


\paragraph{Task 6:} Integration of the proposed core in an architectural simulator (UIUC Kim \& USC Annavaram).
\paragraph{Completion criteria and deliverables:} Memory array circuit schematics, verified by SPICE-accurate simulation.  

Task 2.X: Cache physical design (layout) with fine-grained access and spatiotemporal boosting in a 28nm CMOS technology

Milestone 2.X.1: Demonstration of cache circuits based on the array developed in the Task 1.X. The demonstrated cache circuits can perform narrow- and wide-width data accesses from multiple non-contiguous memory locations. \underline{Seok (Columbia), Kim (UIUC)}

   Completion criteria: functioning cache schematics, layout, and chip prototype

   Approach: If non-contiguous memory locations are in multiple banks, we will parallelize the accesses to hide latency penalty. If non-contiguous memory locations are in the same banks, we will leverage the throughput improvement of the spatiotemporal boosting technique for minimizing latency penalty for accessing non-contiguous memory locations. We will maximize circuit portions that are not boosted such that we can use them as thermal buffer and extend the time and spatial range of supply voltage boosting. 

Task 2.Y: Integration, physical design, and simulations of the accelerator chip in a 28nm CMOS technology. \underline{Seok (Columbia), Kim (UIUC), Hanumolu (UIUC)}

   Completion criteria and deliverables: accelerator chip layout, functionality, performance, and power dissipation verified by post-layout simulations

   Approach: We will integrate the layouts of our custom designed parts (pipelines, caches, links) and third-party IPs (a DRAM controller for Tezzaron's DiRAM). The accelerator will include on-chip testing circuitries including boundary scan-chains, on-chip clock generators, and built-in-self-test (BIST) blocks. We will design the connection structures in the accelerator chip for 2.5D integration. The post-layout netlist will be generated and tested in fast SPICE simulators. 

\noindent
\textbf{Phase 3:}

Task 3.X: Our accelerator chip fabrication and chip-level testing \underline{Seok (Columbia), Kim (UIUC), Hanumolu (UIUC), Annavaram (USC), Hwu (UIUC)}

   Completion criteria: fabricated chips, verified in the chip level

   Approach: We will perform intensive pre-silicon verifications for functional, thermal, voltage integrity, noise, and testability aspects. We will tape-out the accelerator chip in a 28nm CMOS technology. We will test the accelerator chip without DiRAMs. 

Task 3.2: 2.5D integration of our accelerator chip and memory stacks, packaging, and testing \underline{?}

   Completion criteria and deliverables: Functioning 2.5D integrated hardware of the accelerators and DiRAMs. 

   Approach:  We will integrate the accelerator chip with two DiRAMs using a silicon interposer. The silicon interposer that creates 2,000 connection to the accelerator chip per DiRAM will be fabricated by a company (XXXXX). The interposer will be either connected to a custom PCB directly or enclosed on a BGA type package. If necessary, we will mount a off-the-shelf cooler, potentially modified to fit, on top of the accelerator chip. 
